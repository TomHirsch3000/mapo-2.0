
-------------------------------------------------
webhost
-------------------------------------------------
https://mapo.infinityfreeapp.com/
https://dash.infinityfree.com/accounts/if0_40231986/domains/mapo.infinityfreeapp.com

-------------------------------------------------
Start frontend local 
-------------------------------------------------
Open CMD

navigate to the frontend folder 
cd C:\Users\TomHi\Documents\GitHub\mapo-2.0\arxiv-3d-frontend\src

run the javascript app
npm start

-------------------------------------------------
build ready for publishing
-------------------------------------------------
npm run build




-------------------------------------------------
-------------------------------------------------
-------------------------------------------------
run backend process
-------------------------------------------------
-------------------------------------------------
-------------------------------------------------

-------------------------------------------------
open the backend systems
-------------------------------------------------
open ollama -- this is the ai environment

Open CMD

navigate to venv environment -- python environment 
cd C:\Users\TomHi\Documents\GitHub\mapo-2.0\arxiv-3d

start venv
.\venv\Scripts\activate 

open the database browser 
sqllitebrowser

-------------------------------------------------
extract papers
-------------------------------------------------
select a bunch of papers with a filter on 'topic name'

-import_openalex.py

# Example: import 500 highly-cited particle physics papers to a fresh DB
python import_openalex.py ^
  --topic-name "particle physics" ^
  --db papers_particle_physics_100.db ^
  --sample 100000 ^
  --email tom.hirsch3000@gmail.com ^
  --reset


run live batch by year

:: 1900–1950
python import_openalex.py ^
  --topic-name "particle physics" ^
  --db papers_particle_physics_all.db ^
  --from-year 1900 ^
  --to-year 1950 ^
  --sample 0 ^
  --email tom.hirsch3000@gmail.com ^
  --reset

:: 1950–1969
python import_openalex.py ^
  --topic-name "particle physics" ^
  --db papers_particle_physics_all.db ^
  --from-year 1950 ^
  --to-year 1969 ^
  --sample 0 ^
  --email tom.hirsch3000@gmail.com 

:: 1970–1989
python import_openalex.py ^
  --topic-name "particle physics" ^
  --db papers_particle_physics_all.db ^
  --from-year 1969 ^
  --to-year 1989 ^
  --sample 0 ^
  --email tom.hirsch3000@gmail.com 


:: 1990–1999
python import_openalex.py ^
  --topic-name "particle physics" ^
  --db papers_particle_physics_all.db ^
  --from-year 1990 ^
  --to-year 1999 ^
  --sample 0 ^
  --email tom.hirsch3000@gmail.com

:: 2000–2009
python import_openalex.py ^
  --topic-name "particle physics" ^
  --db papers_particle_physics_all.db ^
  --from-year 2000 ^
  --to-year 2009 ^
  --sample 0 ^
  --email tom.hirsch3000@gmail.com

:: 2010–2019
python import_openalex.py ^
  --topic-name "particle physics" ^
  --db papers_particle_physics_all.db ^
  --from-year 2010 ^
  --to-year 2019 ^
  --sample 0 ^
  --email tom.hirsch3000@gmail.com

:: 2020–2025 (or whatever current year)
python import_openalex.py ^
  --topic-name "particle physics" ^
  --db papers_particle_physics_all.db ^
  --from-year 2020 ^
  --to-year 2025 ^
  --sample 0 ^
  --email tom.hirsch3000@gmail.com

-------------------------------------------------
build citations edges
-------------------------------------------------
for all of the papers in the sample, find the citations between them 

# Full rebuild of citations for the big particle-physics DB
python rebuild_citations_openalex.py --db papers_particle_physics.db --reset

# For the smaller 100-sample DB
python rebuild_citations_openalex.py --db papers_particle_physics_100.db --reset

--id-chunk-size 5000 --batch-size 50

python rebuild_citations_openalex.py ^
  --db papers_particle_physics_all.db ^
  --reset ^
  --id-chunk-size 5000 ^
  --batch-size 50


-------------------------------------------------
Find rows missing abstracts from semantic scholar and arxiv
-------------------------------------------------
-fetch_abstracts_s2_arxiv.py

python fetch_abstracts_s2_arxiv.py --db papers_particle_physics.db --verbose --limit 50



-------------------------------------------------
generate AI metadata
-------------------------------------------------

python process_ai_metadata.py --db papers_particle_physics.db --only-unprocessed 1

python process_ai_metadata.py --db papers_particle_physics.db --only-unprocessed 0 --paper-id W1234567890
python process_ai_metadata.py --db papers_particle_physics.db --only-unprocessed 0 --title-contains Higgs
python process_ai_metadata.py --db papers_particle_physics_all.db --limit 10
python process_ai_metadata.py --db papers_particle_physics_all.db --only-unprocessed 0 --limit 10
python process_ai_metadata.py --db papers_particle_physics_all.db --only-unprocessed 1 --min-citations 600

if I just want to process the papers in the node.json then use this
python process_ai_for_landmarks.py --db papers_particle_physics_all.db --from-json nodes.json
-------------------------------------------------
Generate nodes.json + edges.json for the frontend
-------------------------------------------------

python build_frontend_json.py --db papers_particle_physics_all.db

python build_frontend_json.py --db papers_particle_physics_all.db --frontend-dir ../arxiv-3d-frontend/public

python build_frontend_json.py --db papers_particle_physics.db --top-n 1000 --frontend-dir ../arxiv-3d-frontend/public

python build_frontend_json.py ^
    --db papers_particle_physics_all.db ^
    --min-citations 600 ^
    --frontend-dir ../arxiv-3d-frontend/public
    --field "High energy physics" \
    --year-from 1990 --year-to 2020


python build_frontend_json.py ^
  --db papers_particle_physics_all.db ^
  --use-landmarks ^
  --landmark-count 500 ^
  --compute-clusters ^
  --cluster-threshold 3 ^
  --min-citations 50 ^
  --frontend-dir ../arxiv-3d-frontend/public


-------------------------------------------------
build for hosting
-------------------------------------------------






-------------------------------------------------
-------------------------------------------------
-------------------------------------------------
-------------------------------------------------
To do list
-------------------------------------------------
-------------------------------------------------
-------------------------------------------------
-------------------------------------------------


-------------------------------------------------
Backend backlog
-------------------------------------------------
populate summary field with ai process 
-option to overwrite or not
-set output to 16 words
-

pick up field in node json 
-add extra fields to export step

add field to node in frontend 
-read in extra fields from Jason
-design node layout
-3 headings and text: title, ai summary, picture, link, citations, year, authors

include a new node in the database manually 
-put in newton laws of motion 


-------------------------------------------------
front end backlog 
-------------------------------------------------
 - fix axis 
 - add content to nodes add context to nodes 
      -add fields in the database: ai summary, image URL, 
      -add a picture folder in the frontend 

separation experience solve 
-Not jerks [DONE]
-no early separation, accelerating when above a certain size [DONE]
-fix left shift [DONE]
-align pan and scroll movements [DONE]
-simplify code, annotate everything, modularize, drop, Setup git repo [DONE]

-zoom towards pointer, not selected node [DONE]
it didn't work. the nodes are separating at the same rate regardless of sepgain. I think there is some confusion when I talk about separation because I want the zoom to behave in a very specific way. nodes should not move much while they are small and the rate they move should be defined by sepgain, they should only really grow in size. then when they reach a certain size defined by accelfromwidth they should start to move away from the mouse pointer at the rate defined by accelpow. I also want to control the rate that they grow [DONE]
 

 when the graph loads I want the nodes to fade in already in positon, right now they are loading visibly moving and creating the graph in a chaotic way. They should load into position first and then shown and stay still to avioid this loading jumble.
 when I transition to the topic view the galaxy view should disappear, I should only see nodes from within that topic. 

 when I toggle between timeline view and central view the nodes should visibly move a bit slower to their new location, they should not appear to load again.  
 In galaxy view, the bottom pannel should show summary details of the galaxy nodes. 
 I want to rememer the node position, so that if I return to the same view the nodes are in the same place as they were. Right now they are always finding a new position but I want their position to be deterministic. this applies to galaxy nodes and topic nodes 



 -------------------------------------------------
Something old
-------------------------------------------------
in venv run 
-- runs a query against arxiv to extract x papers each with at least y citations 
-- fetch_arxiv_papers -- gets a list of papers on the subject of physics from arxive
-- fetch_arxiv_papers_by_id_list -- gets a the details of papers from arxive given the paperid 
-- clean_arxiv_id -- Normalizes arXiv ID for Semantic Scholar API.
-- get_semantic_scholar_data -- Returns (citationCount, list of cited arXiv IDs)



python extract_source_for_testing.py 

python extract_semantic_scholar.py
--get_semantic_scholar_paper -- Given a Semantic Scholar paperId, fetches metadata, references, and citations.
   ------
   I think I need to process one paper at a time and store the results in a local database or storage solution. 
   I will benefit from validating if I have processed this file before 
      - first include a local lookup 
      - store all the data extracted from semantic scholar including the entire abstract 
    - Run a separate query to process and write it to JSON ready for the frontend

python summarize_semantic_scholar.py
--summarize_text
--AI_category
--AI_category_one
--get_position_from_citations -- Highly cited papers are placed further from center.
--get_size_from_citations

--Output
--node.json -- list of papers with their metadata and position and size based on number of citations
--edges.json -- links between papers by their ID, source, taget, weight

python extract_semantic_scholar.py
-------------------------------------------------